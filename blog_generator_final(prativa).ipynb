{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "utUMjTNVEQFp",
      "metadata": {
        "id": "utUMjTNVEQFp"
      },
      "source": [
        "# ğŸ¦™ Fine-tuning Llama-3.2-3B-Instruct on Blog Dataset\n",
        "> Improved notebook with better LoRA config, prompt engineering, dataset cleaning, and cosine LR scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19DeByTlEQFs",
      "metadata": {
        "id": "19DeByTlEQFs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth gradio transformers datasets trl accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pLUGShTwEQFt",
      "metadata": {
        "id": "pLUGShTwEQFt"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048   # blog posts can be long\n",
        "dtype = None            # auto-detect (bf16 on Ampere+)\n",
        "load_in_4bit = True     # QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I9AAL3IbEQFt",
      "metadata": {
        "id": "I9AAL3IbEQFt"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name   = 'unsloth/Llama-3.2-3B-Instruct',\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p0rs4s1hEQFu",
      "metadata": {
        "id": "p0rs4s1hEQFu"
      },
      "source": [
        "## LoRA Config â€” Key Improvements\n",
        "| Parameter | Before | After | Why |\n",
        "|-----------|--------|-------|-----|\n",
        "| `r` | 16 | 32 | More capacity for long-form generation |\n",
        "| `lora_alpha` | 16 | 64 | Higher alpha â†’ stronger adaptation |\n",
        "| `use_rslora` | False | True | Rank-stabilised LoRA for stability |\n",
        "| `lora_dropout` | 0 | 0.05 | Light regularisation |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9T8y1l2eEQFu",
      "metadata": {
        "id": "9T8y1l2eEQFu"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r              = 32,          # â†‘ from 16 â€“ more expressive\n",
        "    target_modules = [\n",
        "        'q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
        "        'gate_proj', 'up_proj', 'down_proj',\n",
        "    ],\n",
        "    lora_alpha             = 64,      # â†‘ from 16 â€“ stronger signal\n",
        "    lora_dropout           = 0.05,    # light regularisation\n",
        "    bias                   = 'none',\n",
        "    use_gradient_checkpointing = 'unsloth',\n",
        "    random_state           = 3407,\n",
        "    use_rslora             = True,    # â†‘ rank-stabilised LoRA\n",
        "    loftq_config           = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GT6ghd8fEQFv",
      "metadata": {
        "id": "GT6ghd8fEQFv"
      },
      "source": [
        "## Dataset Loading & Cleaning\n",
        "The `source` column is a URL, not meaningful content. We drop it from the prompt and clean whitespace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yP1d3An0EQFv",
      "metadata": {
        "id": "yP1d3An0EQFv"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('nepalprabin/blog_dataset', split='train')\n",
        "print(dataset)\n",
        "print(dataset[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2NZpN7XEQFw",
      "metadata": {
        "id": "c2NZpN7XEQFw"
      },
      "source": [
        "## Improved Prompt Template\n",
        "- Cleaner system instruction focused on blog writing\n",
        "- Removed the noisy `source` (URL) from the training signal\n",
        "- Added section scaffolding hint so model learns structure\n",
        "- Strips extra whitespace from raw text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aq4gkTVEQFw",
      "metadata": {
        "id": "6aq4gkTVEQFw"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    'You are an expert blog writer. '\n",
        "    'Given a title, write a well-structured, engaging blog post in markdown. '\n",
        "    'Include: a compelling introduction, 3-5 body sections with headers, and a strong conclusion.'\n",
        ")\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "Write a blog post with the following title:\n",
        "\n",
        "# {title}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "{content}\"\"\"\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    import re\n",
        "    t = re.sub(r'\\n{3,}', '\\n\\n', t)   # collapse triple newlines\n",
        "    return t.strip()\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for title, content in zip(examples['title'], examples['full_text']):\n",
        "        if not title or not content:          # skip empty rows\n",
        "            continue\n",
        "        text = PROMPT_TEMPLATE.format(\n",
        "            system  = SYSTEM_PROMPT,\n",
        "            title   = clean_text(title),\n",
        "            content = clean_text(content),\n",
        "        ) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {'text': texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
        "print(f'Dataset size after cleaning: {len(dataset)}')\n",
        "print(dataset[0]['text'][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BHMrbedkEQFw",
      "metadata": {
        "id": "BHMrbedkEQFw"
      },
      "source": [
        "## Training Config â€” Key Improvements\n",
        "| Parameter | Before | After | Why |\n",
        "|-----------|--------|-------|-----|\n",
        "| `max_steps` | 60 | 200 | More training for a 3B model |\n",
        "| `lr_scheduler_type` | linear | cosine | Smoother decay, better convergence |\n",
        "| `warmup_ratio` | 5 steps | 0.05 | Proportional warm-up |\n",
        "| `learning_rate` | 2e-4 | 2e-4 | Keep (good default) |\n",
        "| `packing` | False | True | 5Ã— faster for varied-length texts |\n",
        "| `weight_decay` | 0.01 | 0.01 | Keep |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WpliIKAXEQFw",
      "metadata": {
        "id": "WpliIKAXEQFw"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model              = model,\n",
        "    tokenizer          = tokenizer,\n",
        "    train_dataset      = dataset,\n",
        "    dataset_text_field = 'text',\n",
        "    max_seq_length     = max_seq_length,\n",
        "    dataset_num_proc   = 2,\n",
        "    packing            = True,     # â†‘ much faster training\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio       = 0.05,             # â†‘ proportional warm-up\n",
        "        num_train_epochs   = 1,                # â†‘ full epoch instead of fixed steps\n",
        "        max_steps          = 200,              # cap if dataset is large\n",
        "        learning_rate      = 2e-4,\n",
        "        fp16  = not is_bfloat16_supported(),\n",
        "        bf16  = is_bfloat16_supported(),\n",
        "        logging_steps      = 10,\n",
        "        optim              = 'adamw_8bit',\n",
        "        weight_decay       = 0.01,\n",
        "        lr_scheduler_type  = 'cosine',         # â†‘ cosine > linear\n",
        "        seed               = 3407,\n",
        "        output_dir         = 'outputs',\n",
        "        save_strategy      = 'steps',\n",
        "        save_steps         = 50,\n",
        "        report_to          = 'none',\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pt-ZYpK1EQFx",
      "metadata": {
        "id": "pt-ZYpK1EQFx"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()\n",
        "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TL42n8ETEQFx",
      "metadata": {
        "id": "TL42n8ETEQFx"
      },
      "source": [
        "## Save & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w9XwCcM3EQFx",
      "metadata": {
        "id": "w9XwCcM3EQFx"
      },
      "outputs": [],
      "source": [
        "# Save LoRA adapter only (small, fast)\n",
        "model.save_pretrained('blog_writer_lora')\n",
        "tokenizer.save_pretrained('blog_writer_lora')\n",
        "\n",
        "# Optional: merge and save full model (larger, no adapter needed at inference)\n",
        "# model.save_pretrained_merged('blog_writer_merged', tokenizer, save_method='merged_16bit')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xoBv9WnlEQFx",
      "metadata": {
        "id": "xoBv9WnlEQFx"
      },
      "source": [
        "## Inference Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2v0sni-EQFx",
      "metadata": {
        "id": "m2v0sni-EQFx"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(tokenizer, chat_template='llama-3.1')\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def generate_blog(title: str, max_new_tokens: int = 1024, temperature: float = 0.8) -> str:\n",
        "    \"\"\"Generate a blog post for the given title.\"\"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(\n",
        "        system  = SYSTEM_PROMPT,\n",
        "        title   = title,\n",
        "        content = '',          # model fills this in\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').input_ids.to('cuda')\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids      = inputs,\n",
        "            max_new_tokens = max_new_tokens,\n",
        "            use_cache      = True,\n",
        "            temperature    = temperature,\n",
        "            do_sample      = True,\n",
        "            top_p          = 0.9,\n",
        "            repetition_penalty = 1.1,   # reduces repetitive output\n",
        "        )\n",
        "    # decode only the newly generated tokens\n",
        "    new_tokens = output_ids[0, inputs.shape[1]:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# Quick test\n",
        "sample = generate_blog('The Future of Renewable Energy')\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-u2A2TypHGFf",
      "metadata": {
        "id": "-u2A2TypHGFf"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth gradio\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# â”€â”€ Load model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = \"blog_writer_lora\",   # â† change to your saved model path\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit   = True,\n",
        ")\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"âœ… Model loaded\")\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an expert blog writer. \"\n",
        "    \"Given a title, write a well-structured blog post in markdown with \"\n",
        "    \"an introduction, 3-5 sections with headers, and a conclusion.\"\n",
        ")\n",
        "\n",
        "# â”€â”€ Generate function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def generate_blog(title, max_tokens, temperature):\n",
        "    if not title.strip():\n",
        "        return \"Please enter a title.\"\n",
        "\n",
        "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM_PROMPT}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "Write a blog post titled: {title}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids          = inputs,\n",
        "            max_new_tokens     = int(max_tokens),\n",
        "            do_sample          = True,\n",
        "            temperature        = float(temperature),\n",
        "            top_p              = 0.9,\n",
        "            repetition_penalty = 1.1,\n",
        "            use_cache          = True,\n",
        "        )\n",
        "    return tokenizer.decode(output_ids[0, inputs.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "# â”€â”€ Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with gr.Blocks(title=\"Blog Writer\") as demo:\n",
        "    gr.Markdown(\"# âœï¸ AI Blog Writer\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            title   = gr.Textbox(label=\"Blog Title\", placeholder=\"e.g. The Future of AI\")\n",
        "            tokens  = gr.Slider(200, 1200, value=600, step=50, label=\"Length\")\n",
        "            temp    = gr.Slider(0.1, 1.4, value=0.8, step=0.1, label=\"Creativity\")\n",
        "            btn     = gr.Button(\"Generate\", variant=\"primary\")\n",
        "        with gr.Column():\n",
        "            output = gr.Markdown(label=\"Output\")\n",
        "\n",
        "    btn.click(fn=generate_blog, inputs=[title, tokens, temp], outputs=output)\n",
        "    title.submit(fn=generate_blog, inputs=[title, tokens, temp], outputs=output)\n",
        "\n",
        "demo.launch(share=True)   # share=True gives you a public link"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LVX-NYX3EQFx",
      "metadata": {
        "id": "LVX-NYX3EQFx"
      },
      "source": [
        "## Gradio Deployment\n",
        "Run `app.py` (provided separately) in the same environment to launch the web UI.\n",
        "\n",
        "```bash\n",
        "python app.py\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}